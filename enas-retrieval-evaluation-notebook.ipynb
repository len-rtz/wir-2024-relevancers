{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Lab WiSe 2024/2025: Baseline Retrieval System\n",
    "\n",
    "This Jupyter notebook serves as a baseline retrieval system that you can improve upon.\n",
    "We use subsets of the MS MARCO datasets to retrieve passages of web documents.\n",
    "We will show you how to create a software submission to TIRA from this notebook.\n",
    "\n",
    "An overview of all corpora that we use in the current course is available at [https://tira.io/datasets?query=ir-lab-wise-2024](https://tira.io/datasets?query=ir-lab-wise-2024). The dataset IDs for loading the datasets are:\n",
    "\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training`: A subsample of the TREC 2019/2020 Deep Learning tracks on the MS MARCO v1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-rag-20241202-training` (_work in progress_): A subsample of the TREC 2024 Retrieval-Augmented Generation track on the MS MARCO v2.1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/ms-marco-rag-20241203-test` (work in progress): The test corpus that we have created together in the course, based on the MS MARCO v2.1 passage dataset. We will use this dataset as the test dataset, i.e., evaluation scores become available only after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries\n",
    "\n",
    "We will use [tira](https://tira.io/), an information retrieval shared task platform, and [ir_dataset](https://ir-datasets.com/) for loading the datasets. Subsequently, we will build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tira>=0.0.139 ir-datasets python-terrier==0.10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an API client to interact with the TIRA platform (e.g., to load datasets and submit runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset\n",
    "\n",
    "We load the dataset by its ir_datasets ID (as listed in the Readme). Just be sure to add the `irds:` prefix before the dataset ID to tell PyTerrier to load the data from ir_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import get_dataset\n",
    "\n",
    "pt_dataset = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build an index\n",
    "\n",
    "We will then create an index from the documents in the dataset we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training documents:  38%|█▉   | 25828/68261 [00:09<00:12, 3324.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:31.672 [main] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (6114613) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "r-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training documents: 100%|█████| 68261/68261 [00:24<00:00, 2809.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:47.839 [main] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n"
     ]
    }
   ],
   "source": [
    "from pyterrier import IterDictIndexer\n",
    "\n",
    "indexer = IterDictIndexer(\n",
    "    # Store the index in the `index` directory.\n",
    "    \"../data/index\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    ")\n",
    "index = indexer.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the retrieval pipeline\n",
    "\n",
    "We will define a simple retrieval pipeline using just BM25 as a baseline. For details, refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io) or [tutorial](https://github.com/terrier-org/ecir2021tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import terrier\n",
    "\n",
    "bm25 = terrier.Retriever(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the run\n",
    "In the next steps, we would like to apply our retrieval system to some topics, to prepare a 'run' file, containing the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a short look at the first three topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1030303</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037496</td>\n",
       "      <td>who is rep scalise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043135</td>\n",
       "      <td>who killed nicholas ii of russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid                             query\n",
       "0  1030303                who is aziz hashim\n",
       "1  1037496                who is rep scalise\n",
       "2  1043135  who killed nicholas ii of russia"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The `'text'` argument below selects the topics `text` field as the query.\n",
    "pt_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, retrieve results for all the topics (may take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = bm25(pt_dataset.get_topics('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the retrieval. Here are the first 10 entries of the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1030303</td>\n",
       "      <td>53852</td>\n",
       "      <td>8726436</td>\n",
       "      <td>0</td>\n",
       "      <td>31.681671</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1030303</td>\n",
       "      <td>56041</td>\n",
       "      <td>8726433</td>\n",
       "      <td>1</td>\n",
       "      <td>25.966276</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1030303</td>\n",
       "      <td>62116</td>\n",
       "      <td>8726435</td>\n",
       "      <td>2</td>\n",
       "      <td>23.863442</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030303</td>\n",
       "      <td>32183</td>\n",
       "      <td>8726429</td>\n",
       "      <td>3</td>\n",
       "      <td>23.391821</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1030303</td>\n",
       "      <td>35867</td>\n",
       "      <td>8726437</td>\n",
       "      <td>4</td>\n",
       "      <td>21.030669</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1030303</td>\n",
       "      <td>17637</td>\n",
       "      <td>8726430</td>\n",
       "      <td>5</td>\n",
       "      <td>19.967200</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1030303</td>\n",
       "      <td>42957</td>\n",
       "      <td>7156982</td>\n",
       "      <td>6</td>\n",
       "      <td>19.967200</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1030303</td>\n",
       "      <td>21803</td>\n",
       "      <td>8726434</td>\n",
       "      <td>7</td>\n",
       "      <td>19.474804</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1030303</td>\n",
       "      <td>59828</td>\n",
       "      <td>1305520</td>\n",
       "      <td>8</td>\n",
       "      <td>17.849161</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1030303</td>\n",
       "      <td>60002</td>\n",
       "      <td>3302257</td>\n",
       "      <td>9</td>\n",
       "      <td>17.832781</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid  docid    docno  rank      score               query\n",
       "0  1030303  53852  8726436     0  31.681671  who is aziz hashim\n",
       "1  1030303  56041  8726433     1  25.966276  who is aziz hashim\n",
       "2  1030303  62116  8726435     2  23.863442  who is aziz hashim\n",
       "3  1030303  32183  8726429     3  23.391821  who is aziz hashim\n",
       "4  1030303  35867  8726437     4  21.030669  who is aziz hashim\n",
       "5  1030303  17637  8726430     5  19.967200  who is aziz hashim\n",
       "6  1030303  42957  7156982     6  19.967200  who is aziz hashim\n",
       "7  1030303  21803  8726434     7  19.474804  who is aziz hashim\n",
       "8  1030303  59828  1305520     8  17.849161  who is aziz hashim\n",
       "9  1030303  60002  3302257     9  17.832781  who is aziz hashim"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate your run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P_1</th>\n",
       "      <th>P_5</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.412718</td>\n",
       "      <td>0.786653</td>\n",
       "      <td>0.489469</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.62268</td>\n",
       "      <td>0.574227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name       map  recip_rank  ndcg_cut_10       P_1      P_5  \\\n",
       "0  TerrierRetr(BM25)  0.412718    0.786653     0.489469  0.701031  0.62268   \n",
       "\n",
       "       P_10  \n",
       "0  0.574227  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier import Experiment\n",
    "\n",
    "Experiment([bm25],\n",
    "    pt_dataset.get_topics('text'),\n",
    "    pt_dataset.get_qrels(),\n",
    "    eval_metrics = [\"map\", \"recip_rank\", \"ndcg_cut_10\", \"P_1\", \"P_5\", \"P_10\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Improve\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ekhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Function to preprocess text (lowercase and remove punctuation)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to validate documents (filter out very short documents)\n",
    "def is_valid_document(doc):\n",
    "    return len(doc['text'].split()) > 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training documents: 100%|████| 68261/68261 [00:02<00:00, 31823.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned documents: 68253\n",
      "Sample document: {'text': 'voe im really sure voe know need get permit exactly get voe', 'docno': '4459825'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate clean_docs by applying preprocessing functions\n",
    "clean_docs = [\n",
    "    {**doc, 'text': preprocess_text(remove_stopwords(doc['text']))}\n",
    "    for doc in pt_dataset.get_corpus_iter() if is_valid_document(doc)\n",
    "]\n",
    "\n",
    "# Check the number of cleaned documents and inspect a sample\n",
    "print(\"Number of cleaned documents:\", len(clean_docs))\n",
    "print(\"Sample document:\", clean_docs[0])\n",
    "\n",
    "def is_valid_document(doc):\n",
    "    cleaned_text = preprocess_text(remove_stopwords(doc['text']))\n",
    "    return len(cleaned_text.split()) > 5  # Keep only documents with more than 5 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekhar\\AppData\\Local\\Temp\\ipykernel_38132\\985281693.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:18:42.837 [main] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (976964) - further warnings are suppressed\n",
      "12:19:02.968 [main] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "12:19:02.976 [main] ERROR org.terrier.structures.indexing.Indexer -- Could not rename index\n",
      "java.io.IOException: Rename of index structure file 'C:/Users/ekhar/wir-2024-relevancers/data/clean_index/data_1.direct.bf' (exists) to 'C:/Users/ekhar/wir-2024-relevancers/data/clean_index/data.direct.bf' (exists) failed - likely that source file is still open. Possible indexing bug?\n",
      "\tat org.terrier.structures.IndexUtil.renameIndex(IndexUtil.java:379)\n",
      "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:388)\n",
      "Index created successfully!\n",
      "Number of documents: 68253\n",
      "Number of terms: 95646\n",
      "Number of postings: 1565316\n",
      "Number of fields: 1\n",
      "Number of tokens: 2196421\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n",
      "IndexRef type: <class 'jnius.reflect.org.terrier.querying.IndexRef'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Define the index path\n",
    "index_path = \"C:/Users/ekhar/wir-2024-relevancers/data/clean_index\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(index_path, exist_ok=True)\n",
    "\n",
    "def is_valid_document(doc):\n",
    "    # Ensure the document is not empty after cleaning\n",
    "    cleaned_text = preprocess_text(remove_stopwords(doc['text']))\n",
    "    return len(cleaned_text.split()) > 5  # Keep only documents with more than 5 words\n",
    "\n",
    "# Create the index\n",
    "indexer = pt.IterDictIndexer(index_path, meta={'docno': 50, 'text': 4096}, overwrite=True)\n",
    "index = indexer.index(iter(clean_docs))\n",
    "\n",
    "# Print success message\n",
    "print(\"Index created successfully!\")\n",
    "\n",
    "# Load the created index\n",
    "index_object = pt.IndexFactory.of(index_path)\n",
    "\n",
    "# Validate the index by printing collection statistics\n",
    "print(index_object.getCollectionStatistics().toString())\n",
    "\n",
    "# Convert the index path to an IndexRef \n",
    "index_ref = pt.IndexRef.of(index_path)\n",
    "\n",
    "# Print to confirm the type of index_ref\n",
    "print(\"IndexRef type:\", type(index_ref))  # Should print <class 'pyterrier.querying.IndexRef'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Basic BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Results:\n",
      "       qid  docid    docno  rank      score               query\n",
      "0  1030303  53844  8726436     0  32.018835  who is aziz hashim\n",
      "1  1030303  56033  8726433     1  25.228221  who is aziz hashim\n",
      "2  1030303  62108  8726435     2  23.653081  who is aziz hashim\n",
      "3  1030303  32178  8726429     3  23.410157  who is aziz hashim\n",
      "4  1030303  35862  8726437     4  21.360466  who is aziz hashim\n",
      "5  1030303  17634  8726430     5  20.500117  who is aziz hashim\n",
      "6  1030303  42951  7156982     6  20.500117  who is aziz hashim\n",
      "7  1030303  21800  8726434     7  19.706391  who is aziz hashim\n",
      "8  1030303  21333  1305528     8  18.732769  who is aziz hashim\n",
      "9  1030303  59820  1305520     9  17.766626  who is aziz hashim\n"
     ]
    }
   ],
   "source": [
    "# Load the index\n",
    "index = pt.IndexFactory.of(index_path)\n",
    "\n",
    "# Define BM25 retrieval pipeline using the new Retriever API\n",
    "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n",
    "\n",
    "# Load topics (queries) from the dataset\n",
    "topics = pt_dataset.get_topics('text')  # Assuming 'pt_dataset' is your dataset object\n",
    "\n",
    "# Retrieve results for the topics\n",
    "bm25_results = bm25.transform(topics)\n",
    "\n",
    "# Display the top 10 results\n",
    "print(\"BM25 Results:\")\n",
    "print(bm25_results.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval with RM3 Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25+RM3 Results:\n",
      "       qid  docid    docno  rank      score             query_0  \\\n",
      "0  1030303  53844  8726436     0  43.750434  who is aziz hashim   \n",
      "1  1030303  62108  8726435     1  27.309012  who is aziz hashim   \n",
      "2  1030303  56033  8726433     2  25.227136  who is aziz hashim   \n",
      "3  1030303  35862  8726437     3  24.529528  who is aziz hashim   \n",
      "4  1030303  32178  8726429     4  24.270290  who is aziz hashim   \n",
      "\n",
      "                                               query  \n",
      "0  applypipeline:off partner^0.017394459 sy^0.034...  \n",
      "1  applypipeline:off partner^0.017394459 sy^0.034...  \n",
      "2  applypipeline:off partner^0.017394459 sy^0.034...  \n",
      "3  applypipeline:off partner^0.017394459 sy^0.034...  \n",
      "4  applypipeline:off partner^0.017394459 sy^0.034...  \n",
      "Columns in RM3 Results: Index(['qid', 'docid', 'docno', 'rank', 'score', 'query_0', 'query'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define BM25 Retrieval Pipeline\n",
    "bm25 = pt.terrier.Retriever(index_ref, wmodel=\"BM25\")\n",
    "\n",
    "# Step 2: Chain RM3 with BM25\n",
    "bm25_rm3 = bm25 >> pt.rewrite.RM3(index_ref) >> bm25\n",
    "\n",
    "# Step 3: Test RM3 Pipeline\n",
    "topics = pt_dataset.get_topics('text')  # Load topics\n",
    "rm3_results = bm25_rm3.transform(topics)\n",
    "\n",
    "print(\"BM25+RM3 Results:\")\n",
    "print(rm3_results.head())\n",
    "print(\"Columns in RM3 Results:\", rm3_results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "       name       map  recip_rank  ndcg_cut_10       P_1       P_5      P_10\n",
      "0      BM25  0.406065    0.789421     0.489185  0.690722  0.616495  0.569072\n",
      "1  BM25+RM3  0.452028    0.778025     0.523518  0.690722  0.661856  0.616495\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Run Experiment\n",
    "from pyterrier import Experiment\n",
    "\n",
    "results = Experiment(\n",
    "    [bm25, bm25_rm3],  # Compare BM25 and BM25+RM3 pipelines\n",
    "    topics,\n",
    "    pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg_cut_10\", \"P_1\", \"P_5\", \"P_10\"],\n",
    "    names=[\"BM25\", \"BM25+RM3\"]\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Evaluation Results:\n",
      "             name       map  recip_rank  ndcg_cut_10       P_1       P_5  \\\n",
      "0            BM25  0.406065    0.789421     0.489185  0.690722  0.616495   \n",
      "1  BM25+RM3 Tuned  0.441407    0.790812     0.522558  0.721649  0.639175   \n",
      "\n",
      "       P_10  \n",
      "0  0.569072  \n",
      "1  0.609278  \n"
     ]
    }
   ],
   "source": [
    "# Step 5: Tune RM3 Parameters\n",
    "bm25_rm3_tuned = bm25 >> pt.rewrite.RM3(index_ref, fb_terms=15, fb_docs=5, fb_lambda=0.7) >> bm25\n",
    "\n",
    "results_tuned = Experiment(\n",
    "    [bm25, bm25_rm3_tuned],\n",
    "    topics,\n",
    "    pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg_cut_10\", \"P_1\", \"P_5\", \"P_10\"],\n",
    "    names=[\"BM25\", \"BM25+RM3 Tuned\"]\n",
    ")\n",
    "\n",
    "print(\"Tuned Evaluation Results:\")\n",
    "print(results_tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis: BM25 vs. BM25+RM3\n",
    "\n",
    "### **Evaluation Results**\n",
    "| Metric         | BM25     | BM25+RM3  | Improvement (%) |\n",
    "|----------------|----------|-----------|-----------------|\n",
    "| MAP            | 0.406065 | 0.452028  | **+11.3%**      |\n",
    "| Recip Rank     | 0.789421 | 0.778025  | **-1.4%**       |\n",
    "| NDCG@10        | 0.489185 | 0.523518  | **+7.0%**       |\n",
    "| P@1            | 0.690722 | 0.690722  | No change       |\n",
    "| P@5            | 0.616495 | 0.661856  | **+7.4%**       |\n",
    "| P@10           | 0.569072 | 0.616495  | **+8.3%**       |\n",
    "\n",
    "- **MAP** (overall precision) improved by **11.3%**, showing better results with RM3.\n",
    "- **NDCG@10** (top-10 ranking quality) improved by **7.0%**, meaning better-ranked results at the top.\n",
    "- **P@5** and **P@10** (precision at top 5 and 10 results) also improved significantly.\n",
    "- **Recip Rank** dropped slightly by **1.4%**, meaning RM3 sometimes took longer to find the first relevant document.\n",
    "- **P@1** (precision at the first result) stayed the same.\n",
    "\n",
    "---\n",
    "\n",
    "# Tuned Evaluation Results\n",
    "\n",
    "| Metric         | BM25     | BM25+RM3 Tuned | Change vs. Default RM3 |\n",
    "|----------------|----------|----------------|-------------------------|\n",
    "| MAP            | 0.406065 | 0.441407       | **-2.4%**              |\n",
    "| Recip Rank     | 0.789421 | 0.790812       | **+1.6%**              |\n",
    "| NDCG@10        | 0.489185 | 0.522558       | Slight decrease         |\n",
    "| P@1            | 0.690722 | 0.721649       | **+4.5%**              |\n",
    "| P@5            | 0.616495 | 0.639175       | **+3.6%**              |\n",
    "| P@10           | 0.569072 | 0.609278       | **+7.1%**              |\n",
    "\n",
    "- **P@1** (first result relevance) improved by **4.5%**, showing better performance for top results.\n",
    "- **Recip Rank** improved slightly by **1.6%**, meaning the first relevant document was retrieved earlier.\n",
    "- **P@5** and **P@10** also improved, showing better precision at the top results.\n",
    "- **MAP** dropped by **2.4%**, meaning overall relevance slightly decreased.\n",
    "- **NDCG@10** also showed a small drop, indicating some lower-ranked results were less relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The conclusion from the Analysis:**\n",
    "\n",
    "- **BM25+RM3** performed better than BM25 overall, with improvements in MAP (+11.3%) and NDCG@10 (+7.0%), indicating better ranking and retrieval of relevant documents.\n",
    "- Precision at P@5 (+7.4%) and P@10 (+8.3%) also improved, meaning more relevant documents were retrieved in the top 5 and top 10 results.\n",
    "- However, Recip Rank slightly dropped (-1.4%), showing that RM3 occasionally delayed retrieving the first relevant document.\n",
    "\n",
    "For the **tuned BM25+RM3**, precision at P@1 (+4.5%) and Recip Rank (+1.6%) improved, meaning the first relevant document was retrieved earlier and was more accurate. However, MAP (-2.4%) and NDCG@10 saw small declines, suggesting that tuning introduced noise, slightly reducing overall relevance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking with MonoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-terrier==0.12.0\n",
      "  Downloading python_terrier-0.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyterrier_transformers (from versions: none)\n",
      "ERROR: No matching distribution found for pyterrier_transformers\n"
     ]
    }
   ],
   "source": [
    "pip install python-terrier==0.12.0 pyterrier_transformers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
